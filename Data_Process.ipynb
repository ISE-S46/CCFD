{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3db2f6fc-2735-40a9-9f25-7905cb675b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- trans_date_trans_time: timestamp (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- merchant: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- city_pop: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- dob: timestamp (nullable = true)\n",
      " |-- trans_num: string (nullable = true)\n",
      " |-- unix_time: integer (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      " |-- is_fraud: integer (nullable = true)\n",
      "\n",
      "+---+---------------------+----------------+--------------------+--------------+-----+------+--------+------+--------------------+----------+-----+-----+-------+------------------+--------+--------------------+-------------------+--------------------+----------+------------------+-----------+--------+\n",
      "|_c0|trans_date_trans_time|          cc_num|            merchant|      category|  amt| first|    last|gender|              street|      city|state|  zip|    lat|              long|city_pop|                 job|                dob|           trans_num| unix_time|         merch_lat| merch_long|is_fraud|\n",
      "+---+---------------------+----------------+--------------------+--------------+-----+------+--------+------+--------------------+----------+-----+-----+-------+------------------+--------+--------------------+-------------------+--------------------+----------+------------------+-----------+--------+\n",
      "|  0|  2020-06-21 12:14:25|2291163933867244|fraud_Kirlin and ...| personal_care| 2.86|  Jeff| Elliott|     M|   351 Darlene Green|  Columbia|   SC|29209|33.9659|          -80.9355|  333497| Mechanical engineer|1968-03-19 00:00:00|2da90c7d74bd46a0c...|1371816865|         33.986391| -81.200714|       0|\n",
      "|  1|  2020-06-21 12:14:33|3573030041201292|fraud_Sporer-Keebler| personal_care|29.84|Joanne|Williams|     F|    3638 Marsh Union|   Altonah|   UT|84002|40.3207|          -110.436|     302|Sales professiona...|1990-01-17 00:00:00|324cc204407e99f51...|1371816873|39.450497999999996|-109.960431|       0|\n",
      "|  2|  2020-06-21 12:14:53|3598215285024754|fraud_Swaniawski,...|health_fitness|41.28|Ashley|   Lopez|     F|9333 Valentine Point|  Bellmore|   NY|11710|40.6729|          -73.5365|   34496|   Librarian, public|1970-10-21 00:00:00|c81755dbbbea9d5c7...|1371816893|          40.49581| -74.196111|       0|\n",
      "|  3|  2020-06-21 12:15:15|3591919803438423|   fraud_Haley Group|      misc_pos|60.05| Brian|Williams|     M|32941 Krystal Mil...|Titusville|   FL|32780|28.5697|          -80.8191|   54767|        Set designer|1987-07-25 00:00:00|2159175b9efe66dc3...|1371816915|28.812397999999998| -80.883061|       0|\n",
      "|  4|  2020-06-21 12:15:17|3526826139003047|fraud_Johnston-Ca...|        travel| 3.19|Nathan|  Massey|     M|5783 Evan Roads A...|  Falmouth|   MI|49632|44.2529|-85.01700000000001|    1126|  Furniture designer|1955-07-06 00:00:00|57ff021bd3f328f87...|1371816917|         44.959148| -85.884734|       0|\n",
      "+---+---------------------+----------------+--------------------+--------------+-----+------+--------+------+--------------------+----------+-----+-----+-------+------------------+--------+--------------------+-------------------+--------------------+----------+------------------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"FraudDetection\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.csv(\"fraudTest.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show dataset structure\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b5a38ad-ecba-4886-bb6c-41e8780ed897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------+------+--------+--------+------+------+------+------+------+------+------+------+------+------+--------+------+------+---------+---------+---------+----------+--------+\n",
      "|   _c0|trans_date_trans_time|cc_num|merchant|category|   amt| first|  last|gender|street|  city| state|   zip|   lat|  long|city_pop|   job|   dob|trans_num|unix_time|merch_lat|merch_long|is_fraud|\n",
      "+------+---------------------+------+--------+--------+------+------+------+------+------+------+------+------+------+------+--------+------+------+---------+---------+---------+----------+--------+\n",
      "|555719|               555719|555719|  555719|  555719|555719|555719|555719|555719|555719|555719|555719|555719|555719|555719|  555719|555719|555719|   555719|   555719|   555719|    555719|  555719|\n",
      "+------+---------------------+------+--------+--------+------+------+------+------+------+------+------+------+------+------+--------+------+------+---------+---------+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Count missing values\n",
    "df.select([count(col(c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Drop rows with null values (if any)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42da8742-4206-49c8-8460-d52d67bfa90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----+-----------+\n",
      "|trans_date_trans_time|hour|day_of_week|\n",
      "+---------------------+----+-----------+\n",
      "|  2020-06-21 12:14:25|  12|          1|\n",
      "|  2020-06-21 12:14:33|  12|          1|\n",
      "|  2020-06-21 12:14:53|  12|          1|\n",
      "|  2020-06-21 12:15:15|  12|          1|\n",
      "|  2020-06-21 12:15:17|  12|          1|\n",
      "+---------------------+----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, hour, dayofweek\n",
    "\n",
    "# Convert transaction date to timestamp\n",
    "df = df.withColumn(\"trans_date_trans_time\", to_timestamp(col(\"trans_date_trans_time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Extract time-based features\n",
    "df = df.withColumn(\"hour\", hour(col(\"trans_date_trans_time\")))\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(col(\"trans_date_trans_time\")))\n",
    "\n",
    "df.select(\"trans_date_trans_time\", \"hour\", \"day_of_week\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7582076f-0994-4ce0-b484-8183936017d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------------+------------------+\n",
      "|     cc_num|  amt|daily_spending|daily_transactions|\n",
      "+-----------+-----+--------------+------------------+\n",
      "|60416207185| 4.39|          4.39|                 1|\n",
      "|60416207185| 9.33|          9.33|                 1|\n",
      "|60416207185|  3.0|           3.0|                 1|\n",
      "|60416207185|25.04|         25.04|                 1|\n",
      "|60416207185| 5.78|          5.78|                 1|\n",
      "+-----------+-----+--------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum as spark_sum, count as spark_count\n",
    "\n",
    "# Define window by user and transaction date\n",
    "window_spec = Window.partitionBy(\"cc_num\", \"trans_date_trans_time\")\n",
    "\n",
    "# Total daily spending per user\n",
    "df = df.withColumn(\"daily_spending\", spark_sum(\"amt\").over(window_spec))\n",
    "\n",
    "# Transaction count per day\n",
    "df = df.withColumn(\"daily_transactions\", spark_count(\"cc_num\").over(window_spec))\n",
    "\n",
    "df.select(\"cc_num\", \"amt\", \"daily_spending\", \"daily_transactions\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2c5f2aa-ad32-47f2-a5ed-1022b06d55b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------+------------------+\n",
      "|    lat|              long|         merch_lat| merch_long|          distance|\n",
      "+-------+------------------+------------------+-----------+------------------+\n",
      "|33.9659|          -80.9355|         33.986391| -81.200714| 24.56146172635633|\n",
      "|40.3207|          -110.436|39.450497999999996|-109.960431| 104.9250922447634|\n",
      "|40.6729|          -73.5365|          40.49581| -74.196111| 59.08007772921541|\n",
      "|28.5697|          -80.8191|28.812397999999998| -80.883061|27.698567290865142|\n",
      "|44.2529|-85.01700000000001|         44.959148| -85.884734|104.33510630013764|\n",
      "+-------+------------------+------------------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import radians, cos, sin, atan2, sqrt\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in km\n",
    "    return 2 * R * atan2(\n",
    "        sqrt(sin((radians(lat2) - radians(lat1)) / 2) ** 2 +\n",
    "             cos(radians(lat1)) * cos(radians(lat2)) *\n",
    "             sin((radians(lon2) - radians(lon1)) / 2) ** 2),\n",
    "        sqrt(1 - (sin((radians(lat2) - radians(lat1)) / 2) ** 2 +\n",
    "                  cos(radians(lat1)) * cos(radians(lat2)) *\n",
    "                  sin((radians(lon2) - radians(lon1)) / 2) ** 2))\n",
    "    )\n",
    "\n",
    "# Add a new column for distance\n",
    "df = df.withColumn(\"distance\", haversine(col(\"lat\"), col(\"long\"), col(\"merch_lat\"), col(\"merch_long\")))\n",
    "\n",
    "df.select(\"lat\", \"long\", \"merch_lat\", \"merch_long\", \"distance\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a302da2-5b07-4011-939e-66f0da4abe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+----------------+\n",
      "|      category|category_index|category_encoded|\n",
      "+--------------+--------------+----------------+\n",
      "| personal_care|           7.0|  (13,[7],[1.0])|\n",
      "| personal_care|           7.0|  (13,[7],[1.0])|\n",
      "|health_fitness|           9.0|  (13,[9],[1.0])|\n",
      "|      misc_pos|          10.0| (13,[10],[1.0])|\n",
      "|        travel|          13.0|      (13,[],[])|\n",
      "+--------------+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Convert category names to numerical index\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"category_index\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "# One-hot encode category\n",
    "encoder = OneHotEncoder(inputCol=\"category_index\", outputCol=\"category_encoded\")\n",
    "df = encoder.fit(df).transform(df)\n",
    "\n",
    "df.select(\"category\", \"category_index\", \"category_encoded\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "899549db-25cd-42fa-a4e3-ff37bf7b9ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|is_fraud|count|\n",
      "+--------+-----+\n",
      "|       1| 2145|\n",
      "|       0| 2072|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Count fraud and non-fraud transactions\n",
    "fraud_count = df.filter(df.is_fraud == 1).count()\n",
    "non_fraud_count = df.filter(df.is_fraud == 0).count()\n",
    "\n",
    "# Downsample non-fraud transactions\n",
    "fraud_df = df.filter(df.is_fraud == 1)\n",
    "non_fraud_df = df.filter(df.is_fraud == 0).sample(False, fraud_count / non_fraud_count)\n",
    "\n",
    "# Combine balanced dataset\n",
    "df = fraud_df.union(non_fraud_df)\n",
    "\n",
    "# Check final distribution\n",
    "df.groupBy(\"is_fraud\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe518d5-a777-426a-a8e7-40a2869fd6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
