{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a00117-c16c-40d0-a819-2576af2a4658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the PySpark ML Pipeline...\n",
      "Pipeline fitting complete.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, hour, dayofweek,\n",
    "    to_date, datediff, floor,\n",
    "    radians, cos, sin, atan2, sqrt,\n",
    "    sum, count, when, lit\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FraudDetectionPipeline\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"fraudTrain.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Date/Time Features\n",
    "df = df.withColumn(\"trans_date_trans_time\", to_timestamp(col(\"trans_date_trans_time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df = df.withColumn(\"hour\", hour(col(\"trans_date_trans_time\")))\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(col(\"trans_date_trans_time\")))\n",
    "\n",
    "# Daily Spending/Transactions (Window functions)\n",
    "df = df.withColumn(\"trans_date_only\", to_date(col(\"trans_date_trans_time\")))\n",
    "\n",
    "window_spec_daily = Window.partitionBy(\"cc_num\", \"trans_date_only\")\n",
    "\n",
    "df = df.withColumn(\"daily_spending\", sum(\"amt\").over(window_spec_daily))\n",
    "df = df.withColumn(\"daily_transactions\", count(\"cc_num\").over(window_spec_daily))\n",
    "\n",
    "df = df.drop(\"trans_date_only\")\n",
    "\n",
    "# Haversine Distance\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in km\n",
    "    return 2 * R * atan2(\n",
    "        sqrt(sin((radians(lat2) - radians(lat1)) / 2) ** 2 +\n",
    "             cos(radians(lat1)) * cos(radians(lat2)) *\n",
    "             sin((radians(lon2) - radians(lon1)) / 2) ** 2),\n",
    "        sqrt(1 - (sin((radians(lat2) - radians(lat1)) / 2) ** 2 +\n",
    "                  cos(radians(lat1)) * cos(radians(lat2)) *\n",
    "                  sin((radians(lon2) - radians(lon1)) / 2) ** 2))\n",
    "    )\n",
    "\n",
    "df = df.withColumn(\"distance\", haversine(col(\"lat\"), col(\"long\"), col(\"merch_lat\"), col(\"merch_long\")))\n",
    "\n",
    "# Age Calculation\n",
    "df = df.withColumn(\"dob_date\", to_date(col(\"dob\")))\n",
    "df = df.withColumn(\"transaction_date\", to_date(col(\"trans_date_trans_time\")))\n",
    "df = df.withColumn(\"age\", floor(datediff(col(\"transaction_date\"), col(\"dob_date\")) / 365.25))\n",
    "df = df.drop(\"dob_date\", \"transaction_date\")\n",
    "\n",
    "# Class Weight (for handling imbalance)\n",
    "is_fraud_count = df.filter(col(\"is_fraud\") == 1).count()\n",
    "is_not_fraud_count = df.filter(col(\"is_fraud\") == 0).count()\n",
    "\n",
    "class_weight_for_fraud = is_not_fraud_count / is_fraud_count\n",
    "\n",
    "df = df.withColumn(\"class_weight\", when(col(\"is_fraud\") == 1, lit(class_weight_for_fraud)).otherwise(lit(1.0)))\n",
    "\n",
    "# Label preparation (cast to DoubleType, required for MLlib)\n",
    "df = df.withColumn(\"indexedLabel\", col(\"is_fraud\").cast(DoubleType()))\n",
    "\n",
    "# --- Define Pipeline ---\n",
    "stages = []\n",
    "\n",
    "# Categorical Feature Processing (StringIndexer -> OneHotEncoder)\n",
    "categorical_cols = [\"category\", \"gender\", \"merchant\", \"job\"]\n",
    "for col_name in categorical_cols:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_index\", handleInvalid=\"keep\")\n",
    "    encoder = OneHotEncoder(inputCol=f\"{col_name}_index\", outputCol=f\"{col_name}_encoded\")\n",
    "    stages.append(indexer)\n",
    "    stages.append(encoder)\n",
    "\n",
    "# Assemble all features into a single vector\n",
    "feature_cols = [\n",
    "    \"amt\", \"lat\", \"long\", \"city_pop\", \"merch_lat\", \"merch_long\",\n",
    "    \"hour\", \"day_of_week\", \"daily_spending\", \"daily_transactions\",\n",
    "    \"distance\", \"age\"\n",
    "]\n",
    "# Add the newly created one-hot encoded columns\n",
    "for col_name in categorical_cols:\n",
    "    feature_cols.append(f\"{col_name}_encoded\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "stages.append(assembler)\n",
    "\n",
    "# GBT Classifier\n",
    "gbt = GBTClassifier(\n",
    "    labelCol=\"indexedLabel\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"class_weight\",\n",
    "    maxDepth=5,\n",
    "    maxIter=20\n",
    ")\n",
    "stages.append(gbt)\n",
    "\n",
    "# --- Create the Pipeline ---\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "(trainingData, testData) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"Fitting the PySpark ML Pipeline...\")\n",
    "pipeline_model = pipeline.fit(trainingData)\n",
    "print(\"Pipeline fitting complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9fc1b09-ff64-4520-87dc-3f44af13bf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline model saved to: ./kafka/Pipeline\n"
     ]
    }
   ],
   "source": [
    "# --- Save the Trained Pipeline ---\n",
    "model_path = \"./kafka/Pipeline\"\n",
    "pipeline_model.save(model_path)\n",
    "print(f\"Pipeline model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87c2817d-edee-40f8-ab1c-67d0d1c7d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
